{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f0a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyg-nightly in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (2.0.5.dev20220721)\n",
      "Requirement already satisfied: tabulate in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.8.10)\n",
      "Requirement already satisfied: torch in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: sklearn in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: transformers in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (4.16.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: faiss-cpu in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (1.21.2)\n",
      "Requirement already satisfied: folium in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.12.1.post1)\n",
      "Requirement already satisfied: streamlit in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: datasets in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: pickle5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.0.11)\n",
      "Requirement already satisfied: hazm in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (3.3)\n",
      "Requirement already satisfied: tqdm in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (4.63.0)\n",
      "Requirement already satisfied: networkx in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (2.7.1)\n",
      "Requirement already satisfied: spacy in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: en_core_web_sm in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: pyparsing in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pyg-nightly) (3.0.4)\n",
      "Requirement already satisfied: scipy in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pyg-nightly) (1.7.3)\n",
      "Requirement already satisfied: requests in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pyg-nightly) (2.24.0)\n",
      "Requirement already satisfied: jinja2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pyg-nightly) (2.11.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pyg-nightly) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: filelock in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.11.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: torchvision in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.13.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: branca>=0.3.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from folium) (0.5.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (0.7.1)\n",
      "Requirement already satisfied: cachetools>=4.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (4.2.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (3.19.1)\n",
      "Requirement already satisfied: click>=7.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (1.5)\n",
      "Requirement already satisfied: semver in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (2.13.0)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (8.0.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (9.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (4.11.3)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (3.1.27)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (4.2)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (21.4.0)\n",
      "Requirement already satisfied: tornado>=5.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (12.5.1)\n",
      "Requirement already satisfied: pympler>=0.9 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: toml in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: altair>=3.2.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (4.2.0)\n",
      "Requirement already satisfied: validators>=0.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from streamlit) (0.20.0)\n",
      "Requirement already satisfied: aiohttp in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: six in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.16.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: entrypoints in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: toolz in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=1.4->streamlit) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyg-nightly) (2.0.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
      "Requirement already satisfied: ipykernel>=5.1.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pydeck>=0.1.dev5->streamlit) (6.9.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from requests->pyg-nightly) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from requests->pyg-nightly) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from requests->pyg-nightly) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from requests->pyg-nightly) (1.25.11)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from rich>=10.11.0->streamlit) (2.11.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from rich>=10.11.0->streamlit) (0.9.1)\n",
      "Requirement already satisfied: backports.zoneinfo in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from tzlocal>=1.1->streamlit) (0.2.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from validators>=0.2->streamlit) (5.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: joblib in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyg-nightly) (2.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.1.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (8.2.0)\n",
      "Requirement already satisfied: appnope in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: tzdata in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2022.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.0.20)\n",
      "Requirement already satisfied: backcall in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-genutils in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (6.1.4)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.1)\n",
      "Requirement already satisfied: nbconvert in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (6.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (21.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.0.5)\n",
      "Requirement already satisfied: executing in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (21.2.0)\n",
      "Requirement already satisfied: testpath in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: bleach in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.2.1)\n",
      "Requirement already satisfied: defusedxml in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.11)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.21)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyg-nightly tabulate torch sklearn sentencepiece transformers sentence-transformers pandas faiss-cpu numpy folium streamlit datasets pickle5 sklearn hazm nltk tqdm networkx==2.6.3 spacy en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15aa10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import socket, time\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer, SimpleHTTPRequestHandler, test\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, BigBirdModel, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download(\"popular\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "import faiss\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "import json\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "import tqdm\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "%load_ext autoreload\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "import tqdm\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237be963",
   "metadata": {},
   "source": [
    "# HW4\n",
    "## HW4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ecf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw4 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw4.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw4['pos_or_neg'].replace({4: 1}, inplace=True)\n",
    "\n",
    "max_dataset_items = 1000\n",
    "df_hw4 = df_hw4.sample(n=max_dataset_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d2a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: normalize_text(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a65fb",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "### A) regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1686d5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"logistic regression's f1_macro score: 0.42528735632183906\",\n",
       " \"logistic regression's f1_micro data score: 0.74\",\n",
       " \"logistic regression's confusion matrix:\\n[[74  0]\\n [26  0]]\",\n",
       " \"logistic regression's accuracy is: 74.0%\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_hw4 = TfidfVectorizer(ngram_range=(1,2), stop_words = None, norm='l2')\n",
    "x_tfidf_hw4 = vectorizer_hw4.fit_transform(df_hw4.passage)\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf_hw4, df_hw4.pos_or_neg, test_size = 0.1, random_state = 1)\n",
    "regressor_hw4 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(x_train_lr, y_train_lr)\n",
    "\n",
    "def lr_results():\n",
    "    metrics = []\n",
    "    \n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='macro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_macro score: {score}')\n",
    "\n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='micro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_micro data score: {score}')\n",
    "\n",
    "    results = confusion_matrix(y_test_lr, y_predicted)\n",
    "    metrics.append(f'logistic regression\\'s confusion matrix:\\n{results}')\n",
    "\n",
    "    y_test1 = [y for y in y_test_lr]\n",
    "    y_pred1 = [y for y in y_predicted]\n",
    "    counter = 0\n",
    "    for i in range(len(y_test_lr)):\n",
    "      if y_test1[i] == y_pred1[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'logistic regression\\'s accuracy is: {100*counter/len(y_test_lr)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "lr_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6b881",
   "metadata": {},
   "source": [
    "## B) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ffd03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'google/bigbird-base-trivia-itc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Y_hw4 = [y for y in df_hw4.pos_or_neg]\n",
    "X_hw4 = [x for x in df_hw4.passage]\n",
    "x_train_tr, x_testeval_tr, y_train_tr, y_testeval_tr = train_test_split(X_hw4, Y_hw4, test_size = 0.2, random_state = 1)\n",
    "x_test_tr, x_eval_tr, y_test_tr, y_eval_tr = train_test_split(x_testeval_tr, y_testeval_tr, test_size = 0.5, random_state = 1)\n",
    "train_encodings_tr = tokenizer([x for x in x_train_tr], truncation=True, padding=True)\n",
    "eval_encodings_tr = tokenizer([x for x in x_eval_tr], truncation=True, padding=True)\n",
    "test_encodings_tr = tokenizer([x for x in x_test_tr], truncation=True, padding=True)\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings=encodings\n",
    "    self.labels=labels\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_dataset_tr = TweetDataset(train_encodings_tr, y_train_tr)\n",
    "eval_dataset_tr = TweetDataset(eval_encodings_tr, y_eval_tr)\n",
    "test_dataset_tr = TweetDataset(test_encodings_tr, y_test_tr)\n",
    "\n",
    "c1_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49831f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 30\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TweetDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet-classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mc1_model,\n\u001b[1;32m     29\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     33\u001b[0m     )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1339\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1338\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1340\u001b[0m \n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1343\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:359\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:305\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:918\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    911\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 2e-5 \n",
    "weight_decay = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "accumulation_steps = 4\n",
    "num_workers = 10\n",
    "max_len = 2048 \n",
    "metric = load_metric('f1')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"Tweet-classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=lr,\n",
    "    label_smoothing_factor=0.05,\n",
    "    weight_decay=weight_decay,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=c1_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72994edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 13 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-classification', c1_model, tokenizer=tokenizer, config={'max_length':256})\n",
    "y_predicted_transformer = [int(y['label'].split('_')[1]) for y in generator([x for x in x_test])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a57fe5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"transformer model's f1_macro score: 0.44134078212290506 \",\n",
       " \"transformer model's confusion matrix: \\n[[790   0]\\n [210   0]]\",\n",
       " \"transformer model's  accuracy is: 79.0%\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'transformer model\\'s f1_macro score: {f1_score(y_test, y_predicted_transformer, average=\"macro\")} ')\n",
    "    results = confusion_matrix(y_test, y_predicted_transformer)\n",
    "    metrics.append(f'transformer model\\'s confusion matrix: \\n{results}')\n",
    "    counter = 0\n",
    "    for i in range(len(y_test)):\n",
    "      if y_test[i] == y_predicted_transformer[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'transformer model\\'s  accuracy is: {100*counter/len(y_test)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "transformer_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06075416",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### A) K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90067d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSS of our clustering with k-means algorithm is: 651.5501687230692',\n",
       " 'purity of our clustering with k-means algorithm is: 0.7966']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cluster = [x for x in df['passage']]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_cluster = vectorizer.fit_transform(X_cluster)\n",
    "\n",
    "true_k = 5\n",
    "clustering_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "clustering_model.fit(X_cluster)\n",
    "\n",
    "def get_l2_distance(vec1, vec2):\n",
    "  return float(np.dot(vec1-vec2,  (vec1-vec2).T))\n",
    "\n",
    "def calculate_RSS(kmeans_model):\n",
    "  RSS = 0\n",
    "  for point in X:\n",
    "    min_dist = get_l2_distance(kmeans_model.cluster_centers_[0], point)\n",
    "    for center in kmeans_model.cluster_centers_:\n",
    "      if get_l2_distance(point, center) < min_dist:\n",
    "        min_dist = get_l2_distance(point, center)\n",
    "    \n",
    "    RSS += min_dist/15\n",
    "  \n",
    "  return RSS\n",
    "\n",
    "def clustering_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'RSS of our clustering with k-means algorithm is: {calculate_RSS(model)}')\n",
    "\n",
    "    clusters = [[], [], [], [], []]\n",
    "    preds = [x for x in df[\"pos_or_neg\"]]\n",
    "    for i in range(1, len(preds)):\n",
    "      clusters[model.labels_[i]].append(preds[i])\n",
    "\n",
    "    true_elements = 0\n",
    "    for cluster in clusters:\n",
    "      counts = pd.Series(cluster).value_counts()\n",
    "      true_elements += max(counts.get(0), counts.get(1))\n",
    "    metrics.append(f'purity of our clustering with k-means algorithm is: {true_elements/len(preds)}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "clustering_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25074bd9",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "Importing data/preprocessing/using NER to get the named entities and generating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3dde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "require_cols = [0, 1, 8]\n",
    "\n",
    "df_hw5 = pd.read_excel('https://github.com/AmooHashem/my-datasets/raw/main/%40khamenei_ir_user_tweets.xlsx', usecols = require_cols, engine='openpyxl')\n",
    "\n",
    "DOC_NUMBER = len(df_hw5.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfe9d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3167/3167 [00:19<00:00, 159.11it/s]\n",
      "100%|| 3167/3167 [00:00<00:00, 1153770.59it/s]\n"
     ]
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "lemmatizer_hw5 = WordNetLemmatizer()\n",
    "\n",
    "def is_valid_word(text):\n",
    "  return text[0] != '@' and re.search(r\"[^\\w]\", text) is None\n",
    "\n",
    "def normalize_word(text):\n",
    "  text = re.sub(r'[\\.:!\\]\\)\\}\\[\\(\\{]','', text)\n",
    "  text = remove_emojis(text)\n",
    "  return text\n",
    "\n",
    "def normalize_sent(text):\n",
    "  sent = []\n",
    "  for word in text.split():\n",
    "    if is_valid_word(word):\n",
    "      sent.append(normalize_word(word))\n",
    "  sent = ' '.join(sent)\n",
    "  sent = word_tokenize(sent)\n",
    "  sent = ' '.join(sent)\n",
    "  doc = nlp(sent)\n",
    "  sent = [(X.text, X.label_) for X in doc.ents]\n",
    "  return sent\n",
    "\n",
    "\n",
    "data_hw5 = [df_hw5.values[i][1] for i in range(len(df_hw5.values))]\n",
    "messages_hw5 = [x for x in data_hw5]\n",
    "message_entities_hw5 = []\n",
    "for message in tqdm.tqdm(messages_hw5):\n",
    "  message_entities_hw5.append(normalize_sent(str(message)))\n",
    "\n",
    "total_entities_hw5 = set()\n",
    "for entities in tqdm.tqdm(message_entities_hw5):\n",
    "  for entity in entities:\n",
    "    total_entities_hw5.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fa179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hw5 = nx.Graph()\n",
    "G_hw5.add_nodes_from(total_entities_hw5)\n",
    "for entities in message_entities_hw5:\n",
    "  for i in range(len(entities)):\n",
    "    for j in range(i+1, len(entities)):\n",
    "      G_hw5.add_edge(entities[i], entities[j])\n",
    "\n",
    "def sort_tuples(tup):\n",
    "  lst = len(tup)\n",
    "  for i in range(0, lst):\n",
    "    for j in range(0, lst-i-1):\n",
    "      if (tup[j][1] < tup[j + 1][1]):\n",
    "        temp = tup[j]\n",
    "        tup[j]= tup[j + 1]\n",
    "        tup[j + 1]= temp\n",
    "  return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c04861",
   "metadata": {},
   "source": [
    "## Link Analysis\n",
    "### A) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91ed144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1549/1549 [00:00<00:00, 966236.90it/s]\n"
     ]
    }
   ],
   "source": [
    "pr_hw5 = nx.pagerank(G_hw5)\n",
    "entity_prob_hw5 = []\n",
    "\n",
    "for entity in total_entities_hw5:\n",
    "  entity_prob_hw5.append((entity, pr_hw5[entity]))\n",
    "\n",
    "categories_hw5 = ['NORP', 'ORG', 'DATE', 'PERSON', 'GPE', 'LOC', 'CARDINAL']\n",
    "entity_prob_hw5 = sort_tuples(entity_prob_hw5)\n",
    "entity_category_hw5 = {}\n",
    "for entity in tqdm.tqdm(entity_prob_hw5):\n",
    "  for category in categories_hw5:\n",
    "    if entity[0][1] == category:\n",
    "      if category not in entity_category_hw5:\n",
    "        entity_category_hw5[category] = []\n",
    "      if len(entity_category_hw5[category]) < 10:\n",
    "        entity_category_hw5[category].append(entity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3679cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_results():\n",
    "    results = []\n",
    "    for category in entity_category_hw5:\n",
    "        results.append(category)\n",
    "        results.append('_______')\n",
    "        for entity in entity_category_hw5[category]:\n",
    "            results.append(f'-------{entity}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91950551",
   "metadata": {},
   "source": [
    "### A) HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23df27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_results(num_outputs=10):\n",
    "    results = []\n",
    "    h_hw5, a_hw5 = nx.hits(G_hw5)\n",
    "    hubs_hw5 = []\n",
    "    for hub in h_hw5:\n",
    "      hubs_hw5.append((hub, h_hw5[hub]))\n",
    "\n",
    "    hubs_hw5 = sort_tuples(hubs_hw5)\n",
    "\n",
    "    authorities_hw5 = []\n",
    "    for authority in a_hw5:\n",
    "      authorities_hw5.append((authority, a_hw5[authority]))\n",
    "\n",
    "    authorities_hw5 = sort_tuples(authorities_hw5)\n",
    "\n",
    "    results .append(f'Top {num_outputs} hubs: ')\n",
    "    results .append('_____')\n",
    "    for hub in hubs_hw5[:num_outputs]:\n",
    "        results.append('(' + hub[0][0] + ', ' + hub[0][1] + f' ) with score: {hub[1]}')\n",
    "        \n",
    "    results.append('')\n",
    "    results.append('')\n",
    "    results .append(f'Top {num_outputs} authorities: ')\n",
    "    results .append('_____')\n",
    "    for authority in authorities_hw5[:num_outputs]:\n",
    "        results.append('(' + authority[0][0] + ', ' + authority[0][1] + f' ) with score: {authority[1]}')\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c675ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bafdecda",
   "metadata": {},
   "source": [
    "# Setting up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a355ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, query_type, queryExp):\n",
    "    if query_type == 'classification':\n",
    "        return lr_results() + transformer_results()\n",
    "    elif query_type == 'clustering':\n",
    "        return clustering_results()\n",
    "    elif query_type == 'hits':\n",
    "        return hits_results(10)\n",
    "    elif query_type == 'page_rank':\n",
    "        return page_rank_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712ff4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8182c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostName = \"localhost\"\n",
    "serverPort = 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ba431e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started http://localhost:8080\n",
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jul/2022 10:36:33] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'page_rank', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jul/2022 10:36:41] \"POST /page_rank/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped.\n"
     ]
    }
   ],
   "source": [
    "class MyServer(BaseHTTPRequestHandler):\n",
    "    def end_headers (self):\n",
    "        self.send_header('Access-Control-Allow-Origin', '*')\n",
    "        SimpleHTTPRequestHandler.end_headers(self)\n",
    "        \n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-type\", \"text/html\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(bytes(\"<html><head><title>https://pythonbasics.org</title></head>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>Request: %s</p>\" % self.path, \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>This is an example web server.</p>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
    "        \n",
    "    def do_POST(self):\n",
    "        # 1. How long was the message?\n",
    "        length = int(self.headers.get('Content-length', 0))\n",
    "\n",
    "        # 2. Read the correct amount of data from the request.\n",
    "        data = self.rfile.read(length).decode()\n",
    "\n",
    "        # 3. Extract the \"message\" field from the request data.\n",
    "        message = json.loads(data)['query']\n",
    "        print(json.loads(data))\n",
    "        response = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
    "        reply = {'answers': response, 'expanded_query': 'I\\'m an exmanded query and I\\'m very very happy panda!   :D'}\n",
    "        \n",
    "        # Send the \"message\" field back as the response.        \n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/plain; charset=utf-8')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(reply).encode())\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    webServer = HTTPServer((hostName, serverPort), MyServer)\n",
    "    print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
    "\n",
    "    try:\n",
    "        webServer.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    webServer.server_close()\n",
    "    print(\"Server stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5856b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
